\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}                        %
\usepackage[utf8]{inputenc}                  % 
\usepackage{rotating}                        %
%\usepackage{subfigure}                       
%\usepackage[export]{adjustbox}              
\usepackage{bm}


\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}

\DeclareMathOperator{\EX}{\mathbb{E}}% expected value

\ifCLASSOPTIONcompsoc                        %
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
\usepackage[caption=false,font=footnotesize]{subfig}
\fi
%
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\DeclareMathOperator{\traco}{tr}

\begin{document}

\title{Identifying Departures from the Fully Developed Speckle Hypothesis in Intensity SAR Data with Non-Parametric Estimation of the Entropy\\

\thanks{Grantee Capes.}
}

\author{
\begin{minipage}[t]{0.5\textwidth}
\centering
Rosa Janeth Alpala, Abraão D.\ C. Nascimento \\
\textit{Departamento de Estatística} \\
\textit{Universidade Federal de Pernambuco}\\
Recife, PE, Brazil\\
janeth.alpala@ufpe.br, abraao@de.ufpe.br 
\end{minipage}%
%\begin{minipage}[t]{0.3\textwidth}
%\centering
%2nd  Given Name Surname \\
%\textit{Dept. name of organization } \\
%\textit{Name of organization }\\
%City, Country \\
%Email address 
%\end{minipage}%
\begin{minipage}[t]{0.4\textwidth}
\centering
Alejandro C.\ Frery\\
\textit{School of Mathematics and Statistics} \\
\textit{Victoria University of Wellington}\\
Wellington, New Zealand\\
alejandro.frery@vuw.ac.nz
\end{minipage}
}

\maketitle\begin{abstract}
SAR Data are affected by speckle in a non-additive and non-Gaussian way.
The type of distribution these data follow is paramount for their processing and analysis.
Good statistical models provide flexibility and accuracy, often at the cost of using several parameters.
The $\mathcal{G}^0$ distribution is one of the most successful models for SAR data. 
It includes the Gamma law as particular case which arises in the presence of fully developed speckle.
Although the latter is a limit distribution of the former, using the same estimation technique for the more general model is numerically unfeasible.
We propose a two-stage estimation procedure: first, we verify the hypothesis that the data are fully-developed speckle and, if this assumption is rejected, we proceed to estimate the parameters that index the $\mathcal G^0$ distribution.
Given the uncertainty of the underlying distribution, and the negative impact that using an inadequate model has on maximum likelihood estimation, we employ a non-parametric approach to estimate entropy under the fully-developed speckle hypothesis.
\end{abstract}

\begin{IEEEkeywords}
SAR, entropy estimation, non-parametric analysis, order statistics
\end{IEEEkeywords}

\section{Introduction}\label{sec_01}

Remote sensing data from sensors, in particular synthetic aperture radar (SAR), play a key role in understanding the dynamics of the environment, enabling predictive capabilities and facilitating the early detection of disasters. Unlike optical spectrum observations, microwave remote sensing sensors, operating in the microwave band, offer significant advantages, including reduced sensitivity to adverse atmospheric conditions and their self-illuminating properties.

Entropy is a fundamental concept in information theory with broad applications in fields like classification, pattern recognition, statistical physics, stochastic dynamics, and statistics.
Shannon introduced it for a random variables in 1948~\cite{Shannon1948} as a measure of information and uncertainty. 
In statistics, Shannon entropy is a crucial descriptive parameter, particularly for assessing data dispersion and conducting tests for normality, exponentiality, and uniformity~\cite{Wieczorkowski1999}.

Estimating entropy presents practical challenges, especially when the true probability density function (pdf) is unknown.
In such cases, nonparametric methods are employed for entropy estimation. 
In the parametric approach, the form of the pdf is assumed to be known, and its parameters are inferred from the available samples. 
Conversely, nonparametric methods, like histogram or kernel density estimators, do not make such assumptions. 
They estimate the pdf using these techniques and then calculate entropy through numerical or Monte Carlo integration. 
Other nonparametric approaches involve spacing methods, which allow the estimation of the entropy of a random variable with an unknown distribution function when independent and identically distributed (i.i.d.) observations of the random variable are available.

In this paper we consider an estimator based on spacings.

This study is dedicated to the analysis of SAR intensity data, which frequently contends with the presence of speckle noise. 
Speckle noise can significantly complicate the analysis of these data, necessitating the use of specific models and techniques for information extraction. 
We adopt the $\mathcal G^0$ distribution family as a suitable model for describing SAR intensity data, as it effectively characterizes areas with varying degrees of texture.

The article is structured as follows: 
Section~\ref{sec_02} describes 

\section{Statistical modeling for SAR data}\label{sec_02}

\subsection{Intensity data}

The primary models used for intensity SAR data include the Gamma, $\mathcal{K}$, and $\mathcal{G}^0$ distributions. 


\begin{align}
	f(z; \mu, L)&=\frac{L^L}{\Gamma(L)\mu(L)}z^{L-1}\exp\left\{-Lz/\mu\right\},\\
	f(z; \alpha, \gamma, L)&=\frac{L^L\Gamma(L-\alpha)}{\gamma^{\alpha}\Gamma(-\alpha)\Gamma(L)}z^{L-1}(\gamma+Lz)^{\alpha-L}
\end{align}


\section{Nonparametric entropy estimation}



Suppose a random variable $X$ has a distribution function $F(x)$ with a continuous density function $f(x)$. Then, the Shannon entropy $H(X)$ is defined as
\begin{equation*}
  %\label{E:entropy2}
  H(X)=-\int_{-\infty }^\infty \ f(x)\log(f(x))\,dx.
\end{equation*}
The problem of estimating $H(X)$ has been considered by many authors including \cite{vasicek1976test, Bert1992, Wieczorkowski1999, correa1995new}, who proposed estimators based on spacings.

Vasicek \cite{vasicek1976test}, used $f(x)=p$  to express $ H(f)$ as
\begin{equation*}
	H(X)= \int_0^1 \log\left(\frac{d}{dp}Q(p)\right)dp,
\end{equation*}
where $Q(p)=F^{-1}(p)=\inf\left\{x: F(x)\leq p\right\}$ is the quantile function. The derivative of $F^{-1}(p)$ is  estimated by a function of the order statistics.
Assuming that  $\bm{X}=(X_1, X_2, \ldots,X_n)$ is a random sample from the distribution $F(x)$, the estimator is given by
\begin{equation*}
%\label{E:VanEs}
	\widehat{H}_{V}(\bm{X})=\frac{1}{n}\sum_{i=1}^{n}\log\left[\frac{n}{2m}\left(X_{(i+m)}-X_{(i-m)}\right)\right],
	\end{equation*}
where $m<n/2$ is a positive integer, $X_{(i+m)}-X_{(i-m)}$ is the $m$-spacing and $X_{(1)}\leq X_{(2)}\leq\ldots\leq X_{(n)}$ are the order statistics and $X_{(i)}= X_{(1)}$ if $i<1$, $X_{(i)}= X_{(n)}$ if $i>n$.

Multiple authors have presented  adaptations to Vasicek's estimator, including Van Es \cite{Bert1992}, who proposed a new estimator of entropy given by
\begin{multline}
%\label{E:VanEs}
	\widehat{H}_{VE}(\bm{X})=\frac{1}{n-m}\sum_{i=1}^{n-m}\log\left[\frac{n+1}{m}\left(X_{(i+m)}-X_{(i)}\right)\right]\\
	+\sum_{k=m}^n\frac{1}{k}+\log\frac{m}{n+1}.
\end{multline}
Van Es demonstrated that, under some conditions, this estimator exhibits consistency and asymptotic normality.


\section{Fully Developed Speckle}

\section{Results}

\section{Conclusion}\label{sec_09}

In this article, 

\bibliographystyle{IEEEtran}
%\bibliography{strings,refs}

\bibliography{../../Common/references}\end{document}
