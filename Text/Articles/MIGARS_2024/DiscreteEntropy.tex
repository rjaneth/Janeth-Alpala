\documentclass[12pt]{article}

%\usepackage{graphics}
\usepackage{graphicx}
\usepackage{subfigure}
%\usepackage{epsfig}


\usepackage{amsmath}
\usepackage{natbib}
\usepackage{fourier}
\usepackage{xcolor}




%\theoremstyle{plain}% Theorem-like structures
\newtheorem{prop}{Proposition}[section]
\newtheorem{coro}[prop]{Corollary}
\newtheorem{lemma}[prop]{Lemma}



\title{Hypothesis testing based on nonparametric entropies}

\begin{document}
\maketitle
	

Consider the problem of planning hypothesis tests based on nonparametric entropies from the functional \citep{Khashimov1990} and \citep{Bert1992}:
Let $f(\cdot)$ be a density,
\begin{equation}
T_{\mathcal{D}}(f)=\int_{-\infty}^\infty f(x)\Phi(f(x))\mathrm{d}x=\int_{-\infty}^\infty f(x)h\Big(\frac 1{f(x)}\Big)\mathrm{d}x,
\label{Eq:1}
\end{equation}
where $\Phi(x)$ and $h(x)$ are real-valued functions on $[0,\infty)$ satisfying certain regularity conditions.
As an example of \eqref{Eq:1}, the definition of $\Phi(x)=h(1/x)=-\log(x)$ becomes $T_{\mathcal{D}}(f)$ the Shannon entropy formula of a distribution on $(-\infty, \infty)$.
From now on, we use $T_{\mathcal{D}}:=T_{\mathcal{D}}(f)$ as a notation for a possible or well-defined entropy (such as Shannon, R\'enyi, and Tsallis).

A consistent estimator for $T_{\mathcal{D}}$ is given for the next class of statistics:
Let $X_{1:n}\leq X_{2:n}\leq\cdots\leq X_{n:n}$ be the ordered sample from an n-point random sample and $m$ be an integer such that $1\leq m\leq n$,
$$
T_{n,m}
=
{\frac 12}\frac{1}{n-m}\sum_{j=1}^{n-m}\Phi\Big(\frac{m}{n+1}\Big[X_{j+m:n}-X_{j:n}\Big]^{-1}\Big)
=
\frac{1}{n}\sum_{j=1}^{n-m}h\Big(\frac{n}{m}\Big[X_{j+m:n}-X_{j:n}\Big]\Big).
$$
\citet{Khashimov1990} have derived general asymptotic results for functions of spacings; while \citet{Bert1992} have developed a correction for the case of Shannon entropy.]After \citet{Khashimov1990} and \citet{Bert1992}, the next result applies.

\begin{lemma}
Suppose that $f(\cdot)$ is a bounded density bounded away from zero and satisfies a Lipschitz condition on its support.
Then the next asymptotic results follow: If $m,n\rightarrow \infty$ and $m=o(n^{1/2})$, then:
\begin{itemize}
\item[(i)]
$$
\sqrt{n}\,\Big(V^*_{m,n}+\int_{-\infty}^\infty f(x)\log f(x) \mathrm{d}x\Big)
\xrightarrow[]{\mathcal{D}}
\mathcal{N}(0,\operatorname{Var}(\log f(X))),
$$
where $o(\cdot)$ represents the little-o and
$$
V^*_{m,n}=\frac{1}{n-m} \sum_{j=1}^{n-m} \log\Big(\frac{n+1}{m}\Big[X_{j+m:n}-X_{j:n}\Big]\Big)+\sum_{k=m}^n{\frac 1k}+\log m -\log(n+1).
$$
\item[(ii)]
$$
\sigma^{-1}\sqrt{n}\Big(T_{m,n}^*-\int_{-\infty}^{\infty}h(1/f(x))f(x)\mathrm{d}x\Big)
\xrightarrow[]{\mathcal{D}}
\mathcal{N}(0,1),
$$
where $h(1/f(x))=\Phi(f(x))$ provides a unification between the results of \citet{Khashimov1990} and \citet{Bert1992}, $F(x)$ means the cumulative distribution function (cdf) with respect to $f(x)$,
\begin{multline*}
T_{n,m}^*= {\frac 1n}\sum_{j=1}^{n-m}\Bigg[h\Big({\frac nm}\Big[X_{j+m:n}-X_{j:n}\Big]\Big)\\+\frac{[(n/m)(X_{j+m:n}-X_{j:n})]^2}{2m}h''\Big({\frac nm}\Big[X_{j+m:n}-X_{j:n}\Big]\Big)\Bigg]
\end{multline*}
and
\begin{multline*}
\sigma^2=\int_{-\infty}^{\infty}h'\Big(\frac{1}{f(x)}\Big)\frac{1}{f(x)}\mathrm{d}x\\
+\int_{-\infty}^\infty \Big[\int_{-\infty}^y\Big(\frac{1}{f(x)}-\frac{F(x)f'(x)}{f^3(x)}\Big)h'\Big(\frac{1}{f(x)}\Big)f(x)\mathrm{d}x\Big]^2\frac{f(y)}{F^2(y)}\mathrm{d}x
\end{multline*}
for $h'(x)$ and $h''(x)$ as first and second order derivatives, respectively.
\end{itemize}
\end{lemma} 


Let us now consider, starting from the previous lemma, the test of the null hypothesis $\mathcal{H}_0: T_{\mathcal{D}}=D_0$ as opposed to one of the other three:
$$
(i) \mathcal{H}_1:T_{\mathcal{D}}\neq D_0, \quad (ii) \mathcal{H}_1:T_{\mathcal{D}}> D_0, \quad\text{or}\quad (iii) \mathcal{H}_1:T_{\mathcal{D}}< D_0. 
$$
For this purpose, we can use the test statistics:
$$
Z_{m,n} = 
\left\{
\begin{array}{ll}
\frac{\sqrt{n}(V^*_{m,n}-D_0)}{\sqrt{\operatorname{Var}(\log f(X))}}, 
\quad \text{ for }\Phi(x)=\log(x),\\
\frac{\sqrt{n}(T^*_{m,n}-D_0)}{\sigma},
\quad \text{ for any }h(x)\text{ possible}. 
\end{array}
\right.
$$
So the null hypothesis should be rejected if (i) $Z_{n,m} > z_{\alpha/2}$ or $Z_{n,m} < - z_{\alpha/2}$ for $\Phi_{\mathcal N}(z_{\alpha/2})=1-\alpha/2$ and $\Phi_{\mathcal N}$ being the standard normal cdf, (ii) $Z_{n,m} > z_{\alpha/2}$ or (iii) $Z_{n,m} < - z_{\alpha/2}$.  

The power function for case (i) (two-sided test) at $t\neq D_0$ is given by
$$
\pi_{m,n}(t)=1-\Phi_{m,n}\Big(z_{\alpha/2}-\frac{\sqrt{n}(Z_{m,n}-D_0)}{\sigma}\Big)+\Phi_{m,n}\Big(-z_{\alpha/2}-\frac{\sqrt{n}(Z_{m,n}-D_0)}{\sigma}\Big),
$$
for a sequence of cdfs $\Phi_{m,n}(x)$ which tends uniformly to $\Phi_{\mathcal N}(x)$.

\bibliographystyle{agsm}
\bibliography{../../Common/references}
\end{document}