---
title: Identifying Departures from the Fully Developed Speckle Hypothesis in Intensity SAR Data with Non-Parametric Estimation of the Entropy
affiliation:
  
  author-columnar: true # uncomment this line to use
  

  institution:
    - name: Universidade Federal de Pernambuco
      department: Departamento de Estatística
      location: Recife, PE, Brazil
      email: janeth.alpala@ufpe.br, abraao@de.ufpe.br 
      mark: 1
      author:
      - name: Rosa Janeth Alpala, Abraão D.\ C.\ Nascimento 
      
    - name: Victoria University of Wellington
      department: School of Mathematics and Statistics
      location: Wellington, New Zealand
      email: alejandro.frery@vuw.ac.nz
      mark: 2
      author:
        - name: Alejandro C.\ Frery
    

keywords: ["SAR", "entropy estimation", "non-parametric analysis", "order statistics"]
abstract: |
  SAR Data are affected by speckle, a non-additive and non-gaussian interference noise-like pattern.
  The type of distribution these data follow is paramount for their processing and analysis.
  Good statistical models provide flexibility and accuracy, often at the cost of using several parameters.
  The $\mathcal{G}^0$ distribution is one of the most successful models for SAR data. 
  It includes the Gamma law as a particular case which arises in the presence of fully developed speckle.
  Although the latter is a limit distribution of the former, using the same estimation technique for the more general model is numerically unfeasible.
  We propose a two-stage estimation procedure: first, we verify the hypothesis that the data are fully-developed speckle. If this assumption is rejected, we proceed to estimate the parameters that index the $\mathcal G^0$ distribution; otherwise, we proceed with the Gamma model.
  Given the uncertainty of the underlying distribution, and the negative impact that using an inadequate model has on maximum likelihood estimation, we employ a non-parametric approach to estimate entropy under the fully-developed speckle hypothesis.

# use some specific Tex packages if needed. 
#with_ifpdf: true
#with_cite: true
# amsmath need to be true to use with bookdown for referencing equations.
with_amsmath: true
# with_algorithmic: true
# with_array: true
# with_dblfloatfix: true


header-includes:
   - \usepackage[english]{babel}
   - \usepackage{bm,bbm}
   - \usepackage{mathrsfs}
   - \usepackage{siunitx}
   - \usepackage{graphicx}
   - \usepackage{url}
   - \usepackage[T1]{fontenc}
   - \usepackage{polski}
   - \usepackage{booktabs}
   - \usepackage{color}
   - \usepackage{mathtools}
   - \usepackage[utf8]{inputenc}
   - \usepackage{hyperref}
   - \hypersetup{draft}
   - \usepackage{graphicx}
   - \usepackage{float}
   - \usepackage{booktabs}
   - \usepackage{array}
   - \usepackage{multirow}
   - \usepackage{wrapfig}
   - \usepackage{colortbl}
   - \usepackage{pdflscape}
   - \usepackage{xcolor}
   - \usepackage{amsmath}
   - \usepackage{mathabx}
  # - \usepackage{pgf}
   - "\\setcitestyle{square,comma,numbers,sort&compress}" # citation style to numerical, work with \cite{} or [@]


output:
  rticles::ieee_article:
    number_sections: TRUE
    citation_package: natbib

bibliography: ../../Common/references.bib

biblio-style: ieeetr


#link-citations: yes

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE)
library(ggplot2)
library(reshape2)
#library(plotly)
library(knitr)
library(pandoc)
library(gridExtra)
library(gtools)
library(stats4)
library(rmutil)
library(scales)
library(tidyr)
library(rmutil)
library(invgamma)
library(tidyverse)
library(RColorBrewer)
library(ggsci)
#library(wesanderson)
library(ggpubr)
library(patchwork)
library(dplyr)
#options(kableExtra.latex.load_packages = FALSE)
library(devtools)
#devtools::install_github("haozhu233/kableExtra")
library(kableExtra)
library(ggthemes)
library(latex2exp)

theme_set(theme_minimal() +
            theme(text=element_text(family="serif"),
                  legend.position = "bottom")#  top , right , bottom , or left#, panel.grid = element_blank()
)

if(!require("rstudioapi")) install("rstudioapi")
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))


source("../../../Code/R/MainFunctions/gamma_sar_sample.R")
source("../../../Code/R/MainFunctions/entropy_gamma_sar.R")
source("../../../Code/R/MainFunctions/entropy_gI0.R")
source("../../../Code/R/MainFunctions/gi0_sample.R")

source("../../../Code/R/MainFunctions/van_es_estimator.R")
source("../../../Code/R/MainFunctions/correa_estimator.R")
source("../../../Code/R/MainFunctions/bootstrap_correa_estimator_log_mean.R")
source("../../../Code/R/MainFunctions/ebrahimi_estimator.R")
source("../../../Code/R/MainFunctions/noughabi_arghami_estimator.R")
source("../../../Code/R/MainFunctions/vasicek_estimator.R")
source("../../../Code/R/MainFunctions/al_omari_1_estimator.R")
source("../../../Code/R/MainFunctions/al_omari_2_estimator.R")

source("../../../Code/R/MainFunctions/bootstrap_van_es_estimator.R")
source("../../../Code/R/MainFunctions/bootstrap_correa_estimator.R")
source("../../../Code/R/MainFunctions/bootstrap_ebrahimi_estimator.R")
source("../../../Code/R/MainFunctions/bootstrap_noughabi_arghami_estimator.R")
source("../../../Code/R/MainFunctions/bootstrap_vasicek_estimator.R")
source("../../../Code/R/MainFunctions/bootstrap_al_omari_1_estimator.R")
source("../../../Code/R/MainFunctions/bootstrap_al_omari_2_estimator.R")
#The next function contains the functions: generate_samples, calculate_bias_mse, generate_plot
source("../../../Code/R/Programs/functions_sample_bias_mse.R")# 


```



\newtheorem{lemma}{Lemma}

\newcommand{\bias}{\operatorname{Bias}}


# Introduction {#sec:Introduction}

Synthetic aperture radar (SAR) has become a fundamental technology for environmental monitoring and disaster management because of its ability to provide daytime and nighttime imagery in all weather conditions&nbsp;\cite{Mu2019}. 
However, the utility of SAR data depends on a thorough understanding of their statistical properties.
Speckle is part of SAR data because of the imaging process' coherent nature.
Its non-additivity and non-Gaussianity require robust statistical models that can accurately characterize the data.

Among these models, the $\mathcal{G}^0$ distribution stands out as a powerful framework. 
Notably, this distribution encompasses the well-known Gamma distribution as a special case, particularly under the assumption of fully developed speckle. 
The interplay between these two distributions is apparent, with the Gamma distribution representing a limiting case of the more general $\mathcal{G}^0$ model. 

When deciding which model is the best, practitioners face a problem.
On the one hand, if they opt for the Gamma law when the data come from the $\mathcal{G}^0$ distribution, they lose all the information about the number of scatterers, which is revealed by one of the parameters of the latter model&nbsp;\cite{Yue2021}.
On the other hand, if they apply the $\mathcal{G}^0$ distribution under fully developed speckle,
maximum likelihood estimation is tricky: 
bias increases making estimation unreliable&nbsp;\cite{VasconcellosFrerySilva:CompStat}, and
the likelihood is flat, so numerical optimization may not converge&nbsp;\cite{FreryCribariSouza:JASP:04}.
The two-stage technique we propose tackles this problem by using the entropy as a proxy to decide which is the best model.

<!-- The entropy is a fundamental concept in information theory with broad applications to pattern recognition, statistical physics, stochastic dynamics, and statistics. -->
<!-- Shannon introduced it for a random variable in 1948&nbsp;[@Shannon1948] as a measure of information and uncertainty.  -->
<!-- In statistics, Shannon entropy is a crucial descriptive parameter, particularly for assessing data dispersion and conducting tests for normality, exponentiality, and uniformity&nbsp;[@Wieczorkowski1999]. -->

Estimating the entropy faces practical challenges, particularly when the model is unknown; non-parametric methods are utilized in such cases.
Among non-parametric approaches, Subhash et al.&nbsp;\cite{Subhash2021} discussed the use of spacing methods.
This non-parametric strategy offers flexibility to address a wide range of models without imposing specific parametric constraints. We extend the exploration of non-parametric entropy estimators by incorporating enhanced bootstrap methodologies.

<!-- In this context, we present a two-stage estimation procedure designed to navigate the complexities of SAR data.  -->
<!-- The first stage tests the hypothesis that the data follow the Gamma distribution, i.e., we are in the presence of fully developed speckle.  -->
<!-- If this hypothesis is rejected, we proceed to the second stage to estimate the parameters that characterize the $\mathcal{G}^0$ distribution.  --> 

Our aim is to develop a test statistic that helps discriminating between fully-developed speckle and heterogeneous clutter.
We work with non-parametric estimators of the entropy, we reduce the bias of those based on spacings using bootstrap, and select the best ones.
Next, we study the empirical distribution of these estimators under the null hypothesis, followed by a study of the size and power of the proposed test.
We conclude the study with applications to SAR data.

The article is structured as follows: 
Section&nbsp;\ref{sec:Background} covers statistical modeling and entropy estimation for Intensity SAR data. Section&nbsp;\ref{sec:test} outlines hypothesis testing based on non-parametric entropy. 
In Section&nbsp;\ref{sec:results}, we present experimental results. 
Finally, in Section&nbsp;\ref{sec:conclusion} conclusions are exhibited.



# Background {#sec:Background} 

## Statistical modeling of Intensity SAR data 

The primary models used for intensity SAR data include the Gamma and $\mathcal{G}_I^0$  distributions&nbsp;\cite{Frery1997}. 
The first is suitable for fully developed speckle and is a limiting case of the second, which is appealing due to its versatility in accurately representing regions with various roughness characteristics&nbsp;\cite{Cassetti2022}.
We denote $Z \sim \Gamma_{\text{SAR}}(L, \mu)$ and $Z \sim \mathcal{G}_I^0(\alpha, \gamma, L)$ to indicate that $Z$ follows the distributions characterized by the respective probability density functions:
\begin{align}
	f_Z(z;L, \mu)&=\frac{L^L}{\Gamma(L)\mu^L}z^{L-1}\exp\left\{-Lz/\mu\right\} \mathbbm 1_{\mathbbm R_+}(z),\label{E:gamma1}\\
	f_Z(z; \alpha, \gamma, L)&=\frac{L^L\Gamma(L-\alpha)}{\gamma^{\alpha}\Gamma(-\alpha)\Gamma(L)}\cdot\frac{z^{L-1}}{(\gamma+Lz)^{L-\alpha}} \mathbbm 1_{\mathbbm R_+}(z),\label{E:gi01}
\end{align}
where, in&nbsp;\eqref{E:gamma1} $\mu > 0$ is the mean; in&nbsp;\eqref{E:gi01}  $\gamma > 0$ is the scale, $\alpha < -1$ measures the roughness,  $L \geq 1$ is the number of looks, $\Gamma(\cdot)$ is the gamma function, and $\mathbbm 1_{A}(z)$ is the indicator function of the set $A$.

From \eqref{E:gi01}, the $r$th moment of $Z$ is expressed as:
\begin{align}
	\mathbbm E_{\mathcal{G}_I^0}\left(Z^r\right)=\left(\frac{\gamma}{L}\right)^r\frac{\Gamma(-\alpha-r)}{\Gamma(-\alpha)}\cdot\frac{\Gamma(L+r)}{\gamma(L)}, \quad \alpha <-r. 
	\label{E:rmom}
\end{align}
 
Even though the $\mathcal{G}_I^0$  distribution is defined by the parameters $\alpha$ and $\gamma$, SAR literature commonly utilizes the texture $\alpha$ and the mean $\mu$&nbsp;\cite{Nascimento2010}.
In this way, we compute the expected value $\mu$ using the expression in&nbsp;\eqref{E:rmom}, and we reparametrize&nbsp;\eqref{E:gi01} using $\mu$, $\alpha$, and $L$. Then
\begin{align*}
	\mu=\left(\frac{\gamma}{L}\right)\frac{\Gamma(-\alpha-1)}{\Gamma(-\alpha)}\cdot\frac{\Gamma(L+1)}{\gamma(L)}=-\frac{\gamma}{\alpha+1}.
\end{align*}
Thus, the probability density function that characterize the $G_I^0(\mu, \alpha, L)$ law is
\begin{multline}
		f_Z(z; \mu, \alpha, L)=\frac{L^L\Gamma(L-\alpha)}{\big(-\mu(\alpha+1)\big)^{\alpha}\Gamma(-\alpha)\Gamma(L)}\\ \frac{z^{L-1}}{\big(-\mu(\alpha+1)+Lz\big)^{L-\alpha}}.\label{E:gi02}
\end{multline}

## The Shannon Entropy
The parametric representation of Shannon entropy for a system described by a continuous random variable is:
\begin{equation}
  \label{E:entropy2}
  H(Z)=-\int_{-\infty }^\infty \ f(z)\ln f(z)\, \mathrm{d}z,
\end{equation}
here, $f(\cdot)$ is the probability density function that characterizes the distribution of the real-valued random variable $Z$.

Using&nbsp;\eqref{E:entropy2}, we can express the Shannon entropy of $\Gamma_{\text{SAR}}$in&nbsp;\eqref{E:gamma1} and $G_I^0$in&nbsp;\eqref{E:gi02} based on&nbsp;\cite{Cassetti2022} and&nbsp;\cite{Ferreira2020}:
\begin{multline}
\label{E:E-gamma}
H_{\Gamma_{\text{SAR}}}(L, \mu) =   L -\ln L+\ln\Gamma(L)+(1-L)\psi^{(0)}(L) + \ln \mu, 
\end{multline}
\begin{multline}
\label{E:E-GIO}
H_{G_I^0}(\mu, \alpha, L) =L -\ln L+\ln\Gamma(L)+(1-L)\psi^{(0)}(L) +\ln \mu \\
-\ln\Gamma(L-\alpha)+ (L-\alpha) \psi^{(0)}(L-\alpha)\\
-(1-\alpha)\psi^{(0)}(-\alpha)+\ln (-1-\alpha)+\ln\Gamma(-\alpha)-L,
\end{multline}
where $\psi^{(0)}(\cdot)$ is the digamma function. 

In Fig.&nbsp;\ref{fig:PlotGammaSAR} we see how the Entropy of the Gamma SAR distribution changes with $\mu$ for various values of $L$.



```{r PlotGammaSAR, echo=FALSE, message=FALSE, warning=FALSE, out.width="100%", fig.show="hold", fig.align="center", fig.pos="hbt", fig.cap="The Shannon Entropy under the Gamma SAR model."}
L <- c(1, 3, 8, 12, 100)


L_labels <- c(expression(italic(L)==1), 
              expression(italic(L)==3), 
              expression(italic(L)==8), 
              expression(italic(L)==12), 
              expression(italic(L)==100))

mu <- seq(0.1, 10, length.out = 500)


entropies <- sapply(L, function(L) entropy_gamma_sar(L, mu))


muEntropy <- data.frame(mu, entropies)


muEntropy.molten <- melt(muEntropy, id.vars = "mu", variable.name = "Looks", value.name = "Entropy")


ggplot(muEntropy.molten, aes(x = mu, y = Entropy, col = Looks)) +
  geom_line(linewidth=2) +
  theme_minimal() +
  scale_color_manual(values = rainbow(length(L)), labels = L_labels) +
  labs(col = "Looks") +
  xlab(expression(paste(mu))) +
  theme(text=element_text(family="serif"),
        legend.position = "bottom")

```

Fig.&nbsp;\ref{fig:3d_GIO} illustrates the entropy of $G_I^0$ distribution as a function of three key parameters: $\mu$, $\alpha$, and $L$.

```{r 3d_GIO, fig.align="center",  out.width = "70%", fig.cap="The Shannon Entropy under ${G}_I^0$ models.", fig.show="hold",  fig.pos="hbt"}
knitr::include_graphics("../../../Figures/PDF/entropy_plot_3d.pdf")
```
As we explore the 3D plot, we can observe how changes in $\mu$, $\alpha$, and $L$ collectively influence the entropy of the $G_I^0$ distribution. 
We can identify regions where entropy is high or low, providing insights into the predictability and structure of the distribution in various regions of the parameter space.

In Fig.&nbsp;\ref{fig:Plot_GI0_to_gamma}, we depict the behavior of the entropy of \(G_I^0\) when \(\alpha \in \left\{-\infty, -20, -8, -3\right\}\), observing its convergence towards the entropy of \(\Gamma_{\text{SAR}}\) as \(\alpha\) takes large negative values. 


```{r Plot_GI0_to_gamma, echo=FALSE, message=FALSE, warning=FALSE, out.width="100%", fig.show="hold", fig.align="center", fig.pos="hbt", fig.cap="$H_{ G_I^0}$ converges to the $H_{\\Gamma_{\\text{SAR}}}$ as $\\alpha$ takes large negative values and $L=5$."}



L <- c(5)
alphas <- c(  -3, -8, -20, -1000)
alpha_labels <- c( expression(italic(alpha) == -3), expression(italic(alpha) == -8), expression(italic(alpha) == -20), expression(italic(alpha) == -1000))

mu <- seq(0.1, 10, length.out = 500)

# Entropy GI0

muEntropy <- data.frame()

for (alpha in alphas) {
  entropies_GI0 <- sapply(L, function(L) entropy_gI0(mu, alpha, L))
  muEntropy <- rbind(muEntropy, data.frame(mu = mu, Entropy = entropies_GI0, alpha = as.factor(alpha)))
}

muEntropy.molten <- melt(muEntropy, id.vars = c("mu", "alpha"),  value.name = "Entropy")

# Entropy Gamma SAR

entropies_gamma <- sapply(L, function(L) entropy_gamma_sar(L, mu))

Entropy_gamma <- data.frame(mu, entropies_gamma)

Entropy_gamma.molten <- melt(Entropy_gamma, id.vars = "mu", value.name = "Entropy Gamma")

#plot

ggplot(muEntropy.molten, aes(x = mu, y = Entropy, col = alpha)) +
  geom_line(data = Entropy_gamma.molten, aes(x = mu, y = `Entropy Gamma`), color = "blue", linetype = "solid",linewidth = 1.5) + 
  geom_line(linetype = "longdash",  linewidth = 2, alpha=.7) +
  annotate("text", x = max(mu)+0.2, y = max(Entropy_gamma.molten$`Entropy Gamma`), 
           label = TeX("$\\Gamma_{\\tiny{SAR}}$"), vjust = 0.9, hjust = 0.1, color = "blue")+
  theme_minimal() +
  scale_color_manual(values = brewer.pal(7, "Dark2")[1:5], labels = alpha_labels) +
  #scale_color_manual(values = pal_jama()(7)[2:5], labels = alpha_labels) +
  #scale_color_manual(values = 	pal_cosmic()(7)[1:5], labels = alpha_labels) +
  labs(col = "Roughness", linetype = NULL) +
  xlab(expression(paste(mu))) +
  ylab("Entropy") +  
  theme(text = element_text(family = "serif"),
        legend.position = "bottom")


```

The empirical distributions of $\Gamma_{\text{SAR}}$ and $G_I^0$ entropies are shown in Fig.&nbsp;\ref{fig:Plot_empirical}. It is observed that when $\alpha$ approaches negative infinity, the two distributions are very similar. In other words, the mean entropy of $G_I^0$ approximates the mean entropy of $\Gamma_{\text{SAR}}$.

```{r Plot_empirical, echo=FALSE, message=FALSE, warning=FALSE, out.width="100%", fig.show="hold", fig.align="center", fig.pos="hbt", fig.cap="Empirical distributions of $\\Gamma_{\\text{SAR}}$ and $G_I^0$ entropies as $\\mu=1$, $L=5$ and $\\alpha\\in\\{-1000, -20, -8, -3\\}$."}

set.seed(1234567890, kind = "Mersenne-Twister")

R <- 3000
mu <- 1
L <- 5
B <- 1
sample.size <- c( 121)

alpha1_values <- c(-3, -8, -20, -1000)



plots_list <- list()

for (alpha_val in alpha1_values) {

  alpha1 <- alpha_val
  
  TestStatistics1 <- NULL
  TestStatistics2 <- NULL
  
  mean_entropy1 <- numeric(length(sample.size))
  mean_entropy2 <- numeric(length(sample.size))
  
  for (s in sample.size) {
    TestStat1 <- numeric(R)
    TestStat2 <- numeric(R)
    
    for (r in 1:R) {
      z <- gi0_sample(mu, alpha1, L, s)
      TestStat1[r] <- log(mean(z))  + (L - log(L) + lgamma(L) + (1 - L) * digamma(L))
      TestStat2[r] <- log(mean(z))  + (L - log(L) + lgamma(L) +(1 - L) * digamma(L) - L - lgamma(L - alpha1) + (L - alpha1) * (digamma(L - alpha1)) - (1 - alpha1) * digamma(-alpha1) + log(-1 - alpha1) + lgamma(-alpha1))
      
      #TestStat1[r] <- log(mean(z))  - (-L + log(L) - lgamma(L) - (1 - L) * digamma(L))
      #TestStat2[r] <- -bootstrap_correa_estimator_log_mean(z, B) - (-L + log(L) - lgamma(L) - (1 - L) * digamma(L) + L + lgamma(L - alpha1) - (L - alpha1) * (digamma(L - alpha1)) + (1 - alpha1) * digamma(-alpha1) - log(-1 - alpha1) - lgamma(-alpha1))
    }
    
    # mean_entropy1[sample.size == s] <- mean(TestStat1)
    # #cat(" mean for TestStat1", mean_entropy1[sample.size == s], "\n")
    # 
    # mean_entropy2[sample.size == s] <- mean(TestStat2)
    # cat(" mean for TestStat2", mean_entropy2[sample.size == s], "\n")
    
    TestStatistics1 <- rbind(TestStatistics1, data.frame("Sample_Size" = rep(s, R), "Test_Statistics" = TestStat1))
    TestStatistics2 <- rbind(TestStatistics2, data.frame("Sample_Size2" = rep(s, R), "Test_Statistics2" = TestStat2))
  }
  
  max_density1 <- max(density(TestStatistics1$Test_Statistics)$y, na.rm = TRUE)
  max_density2 <- max(density(TestStatistics2$Test_Statistics2)$y, na.rm = TRUE)
  
  plot <- ggplot(TestStatistics1, aes(x = Test_Statistics)) +
    geom_line(aes(col = factor(Sample_Size), linetype = factor(Sample_Size)), stat = "density", linewidth = 1.0) +
    geom_line(data = TestStatistics2, aes(x = Test_Statistics2), stat = "density", color = "blue", linetype = "dashed", linewidth = 1) +
    annotate("text", x = max(TestStatistics1$Test_Statistics) + 0.1, y = max_density1,
             label = TeX("$\\Gamma_{\\tiny{SAR}}$"), vjust = 5, hjust = 5, color = "red") +
    annotate("text", x = max(TestStatistics2$Test_Statistics2) + 0.1, y = max_density2,
             label = TeX("$G_I^0$"), vjust = 5, hjust = 5, color = "blue") +
    labs(x = "Entropy", y = "Empirical Density") +
    annotate("text", x = Inf, y = Inf, label = parse(text = sprintf("alpha == %s", alpha_val)), hjust = 1.2, vjust = 1.02, size = 3) +
    theme_minimal() +
    theme(
      text = element_text(family = "serif"),
      legend.position = "none"  
    )
  
  

  plots_list[[as.character(alpha_val)]] <- plot
}

grid.arrange(grobs = plots_list, ncol = 2)


```
## Estimation of the Shannon Entropy

One of the earliest non-parametric estimators relying on spacings was introduced by&nbsp;\cite{vasicek1976test}. 
Assuming that $\bm{Z}=(Z_1, Z_2,\ldots,Z_n)$ is a random sample from the distribution $F(z)$, the estimator is defined as:
\begin{equation*}
\label{E:Vas}
	\widehat{H}_{\text{V}}(\bm{Z})=\frac{1}{n}\sum_{i=1}^{n}\ln\left[\frac{n}{2m}\left(Z_{(i+m)}-Z_{(i-m)}\right)\right],
	\end{equation*}
where $m<n/2$ is a positive integer, $Z_{(i+m)}-Z_{(i-m)}$ is the $m$-spacing and $Z_{(1)}\leq Z_{(2)}\leq\ldots\leq Z_{(n)}$ are the order statistics and $Z_{(i)}= Z_{(1)}$ if $i<1$, $Z_{(i)}= Z_{(n)}$ if $i>n$.

Several authors have explored adaptations to Vasicek's estimator. In this work, we consider three entropy estimators known for their superior performance:

* \cite{correa1995new}: $\widehat{H}_{\text{C}}$.

* \cite{Ebrahimi1994}: $\widehat{H}_{\mathrm{E}}$.

* \cite{IbrahimAlOmari2014}: $\widehat{H}_{\mathrm{AO}}$.

These estimators, along with others, are described and studied in&nbsp;\cite{Cassetti2022}.
 



## Enhanced Bootstrap Technique

We employ the bootstrap technique to refine the precision of existing non-parametric entropy estimators. 
This approach involves generating new datasets through resampling with repetition from an existing one.

Let's assume that non-parametric entropy estimators \(\widehat{H}=\widehat{\theta}(\bm{Z})\) are inherently biased, that is:
\begin{equation}
\label{Eq:bias1}
\bias\big(\widehat{\theta}(\bm{Z})\big) = E\big[\widehat{\theta}(\bm{Z})\big] - \theta.
\end{equation}
Our objective is to devise unbiased estimators with reduced variance. To achieve this, we introduce an "ideal estimator" \(\check{\theta}(\bm{Z})\) using the bias information:
\begin{equation}
\label{Eq:bias2}
\widecheck{\theta}(\bm{Z}) = \widehat{\theta}(\bm{Z}) - \bias\big(\widehat{\theta}(\bm{Z})\big).
\end{equation}
However, \(\check{\theta}(\bm{Z})\) is not an estimator, because it depends on the true parameter \(\theta\), prompting the formulation of a new estimator \(\widetilde{H}\). 
From \eqref{Eq:bias1} and \eqref{Eq:bias2} we have:
\begin{align*}
\widetilde{H} &= 2\widehat{\theta}(\bm{Z}) - \frac{1}{B}\sum_{b=1}^B \widehat{\theta}_b(\bm{Z}^{(b)}),
\end{align*}
where $B$ is the number of replications in the bootstrap technique.
Applying this methodology, the original estimators by Correa, Ebrahimi, and Al-Omari are now denoted as the proposed bootstrap-enhanced versions: $\widetilde{H}_{\text{C}}$, $\widetilde{H}_{\text{E}}$, and $\widetilde{H}_{\text{AO}}$, respectively.

# Hypothesis testing based on non-parametric entropy {#sec:test}
General asymptotic results for functions of spacings are detailed&nbsp;\cite{Khashimov1990}, while&nbsp;\cite{Bert1992} developed a correction for the case of Shannon entropy.
Following the work of these authors, the next result applies: 
\begin{lemma}
Suppose that $f(\cdot)$ is a bounded density bounded away from zero and satisfies a Lipschitz condition on its support.
Then, if $m,n\rightarrow \infty$ and $m=o(n^{1/2})$, holds that:
\begin{equation*}
\sqrt{n}\,\Big(\label{Eq:bias_t}
\widetilde{H}_{i}+\int_{-\infty}^\infty f(z)\ln f(z) \mathrm{d}z\Big)
\xrightarrow[]{\mathcal{D}}
\mathcal{N}(0,\operatorname{Var}(\ln f(Z))).
\end{equation*}
\end{lemma} 
Consider, starting from the previous lemma, we aim at testing the following hypotheses: 

$$
 \begin{cases}\mathcal{H}_0: \mathcal{D}(\bm{\theta})=\Gamma_{\text{SAR}} \\ 
  \mathcal{H}_1: \mathcal{D}(\bm{\theta})=G_I^0.\end{cases}
$$
In other words, we verify the hypothesis that the data are fully-developed speckle.
<!-- \begin{align*} -->
<!-- \mathcal{H}_1 &: T_{\mathcal{D}}\neq D_0,\\  -->
<!-- \mathcal{H}_1 &: T_{\mathcal{D}}> D_0,\text{ or}\\ -->
<!-- \mathcal{H}_1 &: T_{\mathcal{D}}< D_0.  -->
<!-- \end{align*} -->
For this purpose, we use the following test statistic:
\begin{equation}
\label{Eq:test_e}
S(Z)= \widetilde{H}(Z)-H(Z).
\end{equation}
This test statistic $S(Z)$ aims to assess the behavior of the data under the null hypothesis, through the empirical distribution of the test statistic values for a sample size of $n$. The density should center around zero if the test statistic is capable of detecting that the data are fully-developed speckle, i.e., $S(Z)\approx 0$. Otherwise, under the alternative hypothesis, the empirical distribution of the test statistic values is not centered around zero. This suggests significant differences and implies the presence of heterogeneous clutter.

<!-- so the null hypothesis should be rejected if (i) $Z_{n,m} > z_{\alpha/2}$ or $Z_{n,m} < - z_{\alpha/2}$ for $\Phi_{\mathcal N}(z_{\alpha/2})=1-\alpha/2$ and $\Phi_{\mathcal N}$ being the standard normal cumulative distribution function, (ii) $Z_{n,m} > z_{\alpha/2}$ or (iii) $Z_{n,m} < - z_{\alpha/2}$.   -->


# Results {#sec:results} 

Simulations are conducted using $G_I^0$ distribution, with $200$ simulated samples of size \(n\in\left\{9, 25, 49, 81, 121\right\}\), with parameters \(\mu \in \left\{1, 10\right\}\), \(\alpha=-20\), and \(L=8\). In the case of the bootstrap technique, each sample is replicated $100$ times with replacement.
We choose to use the following heuristic formula for spacing, $m=\left[\sqrt{n}+0.5\right]$.

In Fig. \ref{fig:Plot_bias_mse_gi0} we depict comparisons of bias and mean squared error (MSE) between the original non-parametric entropy estimators and their respective bootstrap-enhanced versions. The use of the bootstrap technique exhibits more precision, reduced bias and MSE, and improved convergence.

The results of simulation are exhibited in Table&nbsp;\ref{tab:table2}. The precision of estimators, as evidenced by bias and MSE comparisons, benefits significantly from the bootstrap technique, particularly for sample sizes below 81.

```{r Simulated_data_gi0, echo=FALSE, message=FALSE}

set.seed(1234567890, kind = "Mersenne-Twister")
sample_sizes <- c(9, 25, 49, 81, 121)

# Number of replications
R <- 100

# Number of bootstrap replications
B <- 10
mu_values <- c(1, 10)
alpha <- -20
L <- 8

estimators <- list(
  "Correa" = correa_estimator,
  "Ebrahimi" = ebrahimi_estimator,
  "Al Omari" = al_omari_1_estimator,
  "Correa Bootstrap" = bootstrap_correa_estimator,
  "Ebrahimi Bootstrap" = bootstrap_ebrahimi_estimator,
  "Al Omari Bootstrap" = bootstrap_al_omari_1_estimator
)


calculate_results_gi0 <- function(sample_sizes, R, B, mu_values, alpha, L, estimators) {
  results_list <- list()

  for (mu_val in mu_values) {
    
    results <- calculate_bias_mse_gi0(sample_sizes, R, B, mu_val, alpha, L, estimators)
    df <- as.data.frame(results)

    
    results_list[[as.character(mu_val)]] <- df
  }

  return(results_list)
}


results_gi0 <- calculate_results_gi0(sample_sizes, R, B, mu_values, alpha, L, estimators)


save(results_gi0, file = "./Data/results_gi0.Rdata")


```


```{r Plot_bias_mse_gi0, echo=FALSE, message=FALSE, warning=FALSE, out.width="100%", fig.show="hold", fig.align="center",  fig.cap="Bias and MSE of entropy estimators for  $G_I^0$, $L=8$, $\\alpha=-20$.", fig.width=6, fig.height=5}


load("./Data/results_gi0.Rdata")


estimators_to_plot <- c("Correa", "Ebrahimi", "Al Omari",  "Correa Bootstrap", "Ebrahimi Bootstrap", "Al Omari Bootstrap" )
  latex_estimator_names <- c("Correa" = expression("$\\widehat{italic(H)}_{C}$"),#parse(text=TeX("$H_{2}$")),
                             "Correa Bootstrap" = expression("$\\widetilde{italic(H)}_{C}$"),
                            "Ebrahimi" = expression("$\\widehat{italic(H)}_{E}$"),
                            "Al Omari" = expression("$\\widehat{italic(H)}_{AO}$"),
                            "Ebrahimi Bootstrap" = expression("$\\widetilde{italic(H)}_{E}$"),
                            "Al Omari Bootstrap" = expression("$\\widetilde{italic(H)}_{AO}$"))
selected_estimators_latex <- latex_estimator_names[estimators_to_plot]


combined_plot_gi0 <- generate_plot_gi0_esp(results_gi0, mu_values, selected_estimators_latex, ncol = 1, nrow = 2)


print(combined_plot_gi0)


```



```{r Table_gi0, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}

load("./Data/results_gi0.Rdata")

estimators_to_table <- c("Correa Bootstrap", "Ebrahimi Bootstrap", "Al Omari Bootstrap")


filtered_results <- purrr::map_dfr(results_gi0, ~ .x %>% filter(Estimator %in% estimators_to_table), .id = "mu")


reshaped_results <- filtered_results %>%
  pivot_wider(names_from = Estimator, values_from = c(Bias, MSE))


colnames(reshaped_results) <- c("$\\bm{\\mu}$", "$\\bm{n}$", "$\\widetilde{H}_{\\text{C}}$", "$\\widetilde{H}_{\\text{E}}$", "$\\widetilde{H}_{\\text{AO}}$", "$\\widetilde{H}_{\\text{C}}$", "$\\widetilde{H}_{\\text{E}}$", "$\\widetilde{H}_{\\text{AO}}$")

#
print(
  kbl(
    reshaped_results,
    caption = "Bias and MSE of bootstrap estimators for $G_I^0$, $L=8$, $\\alpha=-20$.",
    format = "latex",
    booktabs = TRUE,
    align = "c",
    escape = FALSE, 
    digits = 4,  
    label = "table2"  
  ) %>%
    add_header_above(c(" ", " ", "Bias" = 3, "MSE" = 3)) %>%
    collapse_rows(columns = 1:2, latex_hline = "major", valign = "middle")
)
```


```{r Simulated_data_test_gi0, echo=FALSE, message=FALSE}

set.seed(1234567890, kind = "Mersenne-Twister")
sample_sizes <- c(9, 25, 49, 81, 121)
# Number of replications para simulacion montecarlo para calculo de error tipo 1

N <- 1000

# Number of replications para calcular las entropias
R <- 10

# Number of bootstrap replications
B <- 10
mu_values <- c(5)
alpha <- -1000
L <- 2

  estimators <- list(
    #"Correa" = correa_estimator,
    "$\\widetilde{H}_{C}$ " = bootstrap_correa_estimator,
    "$\\widetilde{H}_{E}$ " = bootstrap_ebrahimi_estimator,
    "$\\widetilde{H}_{AO}$ " = bootstrap_al_omari_1_estimator
  )


calculate_results_test_gi0 <- function(sample_sizes, R, B, mu_values, alpha, L, estimators) {
  results_list <- list()

  for (mu_val in mu_values) {
    
    results <- calculate_entropy_and_test(sample_sizes, R, B, mu_val, alpha, L, estimators)
    df <- as.data.frame(results)

    
    results_list[[as.character(mu_val)]] <- df
  }

  return(results_list)
}


results_gi0_1 <- calculate_results_test_gi0(sample_sizes, R, B, mu_values, alpha, L, estimators)


save(results_gi0_1, file = "./Data/results_gi0_1.Rdata")


```


```{r Table_test_gi0, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}

load("./Data/results_gi0_1.Rdata")




for (mu_val in mu_values) {
  results_data <- results_gi0_1[[as.character(mu_val)]]

  
  table_result <- kbl(
    results_data,
    caption = paste("Hypothesis Testing for $G_I^0$, $\\mu =", mu_val, "$, $L=2$, $\\alpha=-1000$ , $H_{\\Gamma_{\\text{SAR}}}=2.493$."),
    format = "latex",
    booktabs = TRUE,
    align = "c",
    escape = FALSE,
    digits = 4,
    label = "table_hipotesis"
    #label = paste("table_hipotesis", mu_val, sep = "")
  ) %>%
    collapse_rows(columns = 1:3, latex_hline = "major", valign = "middle")

  
  print(table_result)
}

```

Fig.&nbsp;\ref{fig:Plot_empirical_test} displays the empirical distributions using statistical test \eqref{Eq:test_e}. Under the null hypothesis, we observe that the densities are centered around zero. Conversely, under the alternative hypothesis, such as for $\alpha=8$, the density is not centered around zero. It reveals the difference between the entropy of Gamma SAR and $G_I^0$, indicating that we are in a scenario of heterogeneous clutter.

```{r Plot_empirical_test, echo=FALSE, message=FALSE, warning=FALSE, out.width="100%", fig.show="hold", fig.align="center", fig.pos="hbt", fig.cap="Empirical distributions under null hypothesis (centered around zero) and alternative hypothesis (e.g., $\\alpha=8$, showing a shift from zero), with $\\mu=1$ and $L=5$."}

set.seed(1234567890, kind = "Mersenne-Twister")

R <- 3000
mu <- 1
L <- 5
B <- 2
sample.size <- c( 81)

alpha1_values <- c( -8)


plots_list <- list()

for (alpha_val in alpha1_values) {

  alpha1 <- alpha_val
  
  TestStatistics1 <- NULL
  TestStatistics2 <- NULL
  
  mean_entropy1 <- numeric(length(sample.size))
  mean_entropy2 <- numeric(length(sample.size))
  
  for (s in sample.size) {
    TestStat1 <- numeric(R)
    TestStat2 <- numeric(R)
    
    for (r in 1:R) {
      z <- gi0_sample(mu, alpha1, L, s)
      TestStat1[r] <- bootstrap_correa_estimator_log_mean(z, B) - (L - log(L) + lgamma(L) + (1 - L) * digamma(L))
      TestStat2[r] <- bootstrap_correa_estimator_log_mean(z, B) - (L - log(L) + lgamma(L) + (1 - L) * digamma(L) - L - lgamma(L - alpha1) + (L - alpha1) * (digamma(L - alpha1))- (1 - alpha1) * digamma(-alpha1) + log(-1 - alpha1) + lgamma(-alpha1))
      
    }
    
    mean_entropy1[sample.size == s] <- mean(TestStat1)
   # cat(" mean for TestStat1", mean_entropy1[sample.size == s], "\n")
    
    mean_entropy2[sample.size == s] <- mean(TestStat2)
   # cat(" mean for TestStat2", mean_entropy2[sample.size == s], "\n")
    
    TestStatistics1 <- rbind(TestStatistics1, data.frame("Sample_Size" = rep(s, R), "Test_Statistics" = TestStat1))
    TestStatistics2 <- rbind(TestStatistics2, data.frame("Sample_Size2" = rep(s, R), "Test_Statistics2" = TestStat2))
  }
  
  difference.betweeen.GammaSAR.GI0.n.Look <- function(alpha, L) {
    return(
      -L - lgamma(L-alpha) + 
        (L-alpha)*(digamma(L - alpha))- 
        (1-alpha)*digamma(- alpha)+
        log(-1 - alpha)+
        lgamma(-alpha)
    )
  }
  
  max_density1 <- max(density(TestStatistics1$Test_Statistics)$y, na.rm = TRUE)
  max_density2 <- max(density(TestStatistics2$Test_Statistics2)$y, na.rm = TRUE)
  
  plot <- ggplot(TestStatistics1, aes(x = Test_Statistics)) +
    geom_line(aes(col = factor(Sample_Size), linetype = factor(Sample_Size)), stat = "density", linewidth = 1.0) +
    geom_line(data = TestStatistics2, aes(x = Test_Statistics2), stat = "density", color = "blue", linetype = "dashed", linewidth = 1) +
    geom_vline(xintercept = difference.betweeen.GammaSAR.GI0.n.Look(-8, 5), col="darkgreen", linetype = "dotted", linewidth = 0.6) +
    annotate("text", x=difference.betweeen.GammaSAR.GI0.n.Look(-8, 5), y=7, parse=TRUE, label="alpha==-8", hjust=-.1, vjust=1.0, col="darkgreen") +
   # geom_vline(xintercept = difference.betweeen.GammaSAR.GI0.n.Look(-20, 5), col="darkgreen", linetype = "dotted", linewidth = 0.6) +
   # annotate("text", x=difference.betweeen.GammaSAR.GI0.n.Look(-20, 5), y=7, parse=TRUE, label="alpha==-20", hjust=-.1, vjust=2.0, col="darkgreen") +
    geom_vline(xintercept = difference.betweeen.GammaSAR.GI0.n.Look(-1000, 5), col="darkgreen", linetype = "dotted", linewidth = 0.6) +
    #annotate("text", x=difference.betweeen.GammaSAR.GI0.n.Look(-1000, 5), y=7, parse=TRUE, label="alpha==-1000", hjust=-.1, vjust=4.0, col="darkgreen") +
    labs(x = "Test Statistics", y = "Empirical Density") +
    #annotate("text", x = Inf, y = Inf, label = parse(text = sprintf("alpha == %s", alpha_val)), hjust = 1.2, vjust = 1.02, size = 3) +
    theme_minimal() +
    theme(
      text = element_text(family = "serif"),
      legend.position = "none"  
    )
  
  

  plots_list[[as.character(alpha_val)]] <- plot
}

grid.arrange(grobs = plots_list, ncol = 1)

```

A hypothesis testing was conducted using non-parametric entropy estimators as test statistics. 
The results in Table&nbsp;\ref{tab:table_hipotesis} show that data from the $G_I^0$ distribution exhibit fully developed speckle behavior, specifically in the limit case with parameters $\mu=5$, $L=2$, and $\alpha= -1000$. 
The true entropy of $\Gamma_{\text{SAR}}$ was set at $H_{\Gamma_{\text{SAR}}} = 2.493$.  
It is observed that, for different sample sizes, entropy values converge towards the true value of the Gamma SAR distribution. 

```{r Simulated_data_test_gi0_2, echo=FALSE, message=FALSE}

set.seed(1234567890, kind = "Mersenne-Twister")
sample_sizes <- c(49, 81, 121)
N<- 100
# Number of replications
R <- 100

# Number of bootstrap replications
B <- 2
mu_values <- c(1)
alpha <- -600
L <- 5

  estimators <- list(
  
    "$\\widetilde{H}_{C}$ " = bootstrap_correa_estimator
    #"$\\widetilde{H}_{E}$ " = bootstrap_ebrahimi_estimator,
    #"$\\widetilde{H}_{AO}$ " = bootstrap_al_omari_1_estimator
  )
 #true_entropy <- entropy_gamma_sar(L, mu) 
calculate_entropy_and_test_montecarlo <- function(sample_sizes, R, N, B, mu, alpha, L, estimators, true_entropy) {
  true_entropy <- entropy_gamma_sar(L, mu)
  output <- data.frame(
    SampleSize = integer(0),
    Estimator = character(0),
    TypeIErrorRate = numeric(0)
  )
  
  for (ssize in sample_sizes) {
    
    for (estimator_name in names(estimators)) {
      estimator <- estimators[[estimator_name]]
      reject_H0_count <- 0
      
      for (n_replication in 1:N) {
        
        #samples <- gi0_sample(mu, alpha, L, ssize)
        v.entropy <- numeric(R)
        for (r in 1:R) {
       
        samples <- gi0_sample(mu, alpha, L, ssize)
        
        if (grepl(" ", estimator_name)) {
          v.entropy[r] <- estimator(samples, B = B)
        } else {
          v.entropy[r] <- estimator(samples)
        }
      }
        

        mean_entropy <- mean(v.entropy)
      z_statistic <- sqrt(R) * (mean_entropy - true_entropy) / sd(v.entropy)
      p_value <- 2 * (1 - pnorm(abs(z_statistic)))
      alpha_t = 0.05
        

        reject_H0_count <- reject_H0_count + sum(p_value < 0.05)
      }
      
      
      typeI_error_rate <- reject_H0_count / (N)
      
      output <- rbind(
        output,
        data.frame(
          SampleSize = ssize,
          Estimator = estimator_name,
          TypeIErrorRate = round(typeI_error_rate, 5)
        )
      )
    }
  }
  
  colnames(output) <- c("$n$", "Estimator", "Type I Error Rate")
  
  return(output)
}



calculate_results_test_gi0_montecarlo <- function(sample_sizes, R, N, B, mu_values, alpha, L, estimators) {
  results_list <- list()

  for (mu_val in mu_values) {
    
    results <- calculate_entropy_and_test_montecarlo(sample_sizes, R, N, B, mu_val, alpha, L, estimators)
    df <- as.data.frame(results)

    results_list[[as.character(mu_val)]] <- df
  }

  return(results_list)
}

results_gi0_1_montecarlo <- calculate_results_test_gi0_montecarlo(sample_sizes, R, N, B, mu_values, alpha, L, estimators)
#print(results_gi0_1_montecarlo)
save(results_gi0_1_montecarlo, file = "./Data/results_gi0_1_montecarlo.Rdata")


```

```{r Table_test_gi0_montecarlo, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
load("./Data/results_gi0_1_montecarlo.Rdata")


for (mu_val in mu_values) {
  results_data <- results_gi0_1_montecarlo[[as.character(mu_val)]]

  table_result <- kbl(
    results_data,
    caption = paste("Type I error with $L=5$, $\\mu=1$ and$\\alpha=\\infty$."),
    format = "latex",
    booktabs = TRUE,
    align = "c",
    escape = FALSE,
    digits = 4,
    label = "table_hipotesis_2"
  ) %>%
    collapse_rows(columns = 1:2, latex_hline = "major", valign = "middle")

  #cat("### Results for mu =", mu_val, "\n")
  print(table_result)
  
}





```
Hypothesis test results were conducted with a \SI{95}{\percent} confidence level. 
The $Z$ statistic measures the discrepancy between the estimated entropy and the true entropy. 
The $p$-values associated with the $Z$ statistic are predominantly greater than $0.05$, for sample sizes above $25$, suggesting that the data are consistent with the null hypothesis.


# Conclusion {#sec:conclusion} 


In this study, three estimators renowned for their robust performance across diverse distributions were chosen for evaluation. We examine the impact of applying bootstrap techniques on their precision by comparing each estimator's performance with its bootstrap-enhanced counterpart, utilizing metrics such as bias and MSE. The effectiveness of bootstrap-enhanced non-parametric entropy estimators was observed, demonstrating efficacy in most instances. Nonetheless, it is essential to recognize that the applicability of this technique may not be universal across all estimation methods.

It is worth noting that this analysis represents an initial exploration of SAR Intensity data, and future work will include a more in-depth analysis of effect size and statistical power.
Additionally, exploring the impact of the $\alpha$ parameter on estimates and conducting more extensive analyses to assess the generalization of results to different parameter configurations is suggested.




# References {#references .numbered}


